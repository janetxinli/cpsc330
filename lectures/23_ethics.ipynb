{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 Lecture 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "- Announcements + survey (5 min)\n",
    "- Ethics (15 min)\n",
    "- Ethics activity (20 min)\n",
    "- Break (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder to self: **turn on recording!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements + survey (5 min)\n",
    "\n",
    "- [CPSC 330 exit survey](https://ubc.ca1.qualtrics.com/jfe/form/SV_e9EDuThtRBbBaJL).\n",
    "- hw8 due this Thursday, April 9 at 6pm.\n",
    "- No more tutorials.\n",
    "- This is the last lecture.\n",
    "- Final exam is still April 24 from 12-2:30pm.\n",
    "  - Tentative plan: similar to an assignment, open-book, but time-limited and no collaboration allowed.\n",
    "  - Rationale: would do a reasonable job of testing the main points of the course; it would be harder to cheat than a closed-book exam.\n",
    "  - Finalized information to follow soon.\n",
    "- We have added 10 office hours during April 8-23; see the [calendar](https://www.cs.ubc.ca/~mgelbart/calendar.html).\n",
    "  - These will take place on Collaborate Ultra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "Ethics, Fairness, Safety, Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics for ML (15 min)\n",
    "\n",
    "- Today I will attempt to say a bit about ethics, per your request at the start of the course. \n",
    "- Warning: I do not know anything about ethics.\n",
    "- I looked through the topics of CPSC 430 (Computers and Society) and did not find much about ML.\n",
    "  - Thanks to Kevin L-B for sharing these.\n",
    "- I had a bit more luck with DSCI 541 (Privacy, Ethics and Security) from the MDS program.\n",
    "  - Thanks to Ed Knorr for sharing these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Privacy and ethics issues around data have been in the news a lot lately. E.g. the [Cambridge Analytica scandal](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal) - there's even a [Netflix movie](https://www.netflix.com/ca/title/80117542) about it.\n",
    "\n",
    "We saw various eyebrow-raising results in previous lectures:\n",
    "\n",
    "- in lecture 5 with the movie review dataset, the feature \"women\" had a negative logistic regression coefficient.\n",
    "- in lecture 14 on deep learning, we discussed:\n",
    "  - the model picking up on the wrong features, e.g. \n",
    "    - incorrectly predicting sports based on a person's race\n",
    "    - the word \"PORTABLE\" for diagnosing cardiomegaly\n",
    "  - adversarial examples specifically designed to fool a neural network\n",
    "  - deepfakes\n",
    "- in lecture 15 on natural language data, we saw inherently biased word embeddings.\n",
    "- maybe more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be various problems here:\n",
    "\n",
    "- Bias/fairness\n",
    "- AI safety\n",
    "- Privacy\n",
    "- and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study: criminal machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calling BS video 5.5 on [\"Criminal Machine Learning\"](https://www.youtube.com/watch?v=rga2-d1oi30&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=27) (pun intended, presumably?) discussed a case of predicting whether someone is a criminal based on their face.\n",
    "- There is also a text case study [here](https://callingbullshit.org/case_studies/case_study_criminal_machine_learning.html) in which they critique (criticize?) the work.\n",
    "- They show some sample training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://callingbullshit.org/case_studies/img/criminals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your prediction algorithm is only as good (or as bad) as your traning data.\n",
    "- Possible problems: \n",
    "  - Wearing a white shirt and jacket vs. other clothes (turns out the authors removed these).\n",
    "  - Facial expressions (e.g. smiling vs. frowning) - this is what the Calling BS folks think is happening.\n",
    "  - Cropping, lighting, etc.\n",
    "- It would be interesting to try something like SHAP or LIME here to highlight what areas of the image are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But the Calling BS post also raises a fundamentally different type of problem:\n",
    "  - What if the criminal justice system was biased in the first place?\n",
    "  - E.g. it tends to convict less attractive faces.\n",
    "  - Then the algorithm will learn to emulate this.\n",
    "- It would be challenging to get around this problem.\n",
    "- How can our algorithm be \"better\" (less biased) than humans if humans labeled the data?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a step back here: \n",
    "\n",
    "- How much \"signal\" do we expect to get from faces anyway? \n",
    "  - I would say, maybe a tiny bit better than random. \n",
    "- The authors claim 90% accuracy. That sounds too good to be true. Per the Calling BS videos, if it sounds too good to be true, it probably is.\n",
    "- Even if the results seem to make sense, do the risks outweigh the benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suggested strategies: experimental set-up problems\n",
    "\n",
    "- Ask yourself, are my results too good to be true?\n",
    "  - e.g. ![](img/piazza_tgtbt2.png)\n",
    "  - The OP did the right thing here by questioning the results.\n",
    "- Use baselines like `DummyClassifier`.\n",
    "- Look at feature importances.\n",
    "- Manually look at some of the correct/incorrect predictions (or very low/high error for regression).\n",
    "- Try making changes or perturbations (e.g. different train/validation folds) and check if your results are robust.\n",
    "- When you are done, think carefully about your confidence (or _credence_, see lecture 22) regarding any claims you make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suggested strategies: ethical/fairness issues\n",
    "\n",
    "- Bias usually comes from the data, not the algorithm. Think carefully about how the training data were collected.\n",
    "- Familiarize yourself with how your model will be used.\n",
    "- Ask yourself who might be affected by this model in deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics activity (20 min)\n",
    "\n",
    "**Background info.** University admissions offices are known to use ML (often through a specialized 3rd party company) as part of their admissions process. Here are some things they might want to predict (this is a mix of [things](https://www.nytimes.com/interactive/2019/09/10/magazine/college-admissions-paul-tough.html) I've [read](https://www.washingtonpost.com/business/2019/10/14/colleges-quietly-rank-prospective-students-based-their-personal-data/) and some speculation):\n",
    "\n",
    "- What is this student's chance of accepting an offer from us?\n",
    "- How much financial aid do we need to give this student in order for them to accept?\n",
    "- How well might the student do academically at our institution? How likely are they to graduate within 6 years?\n",
    "- Aside from tuition: how likely would the student be to donate in the future? In short, what is their [customer lifetime value](https://en.wikipedia.org/wiki/Customer_lifetime_value)? (We briefly mentioned this in lecture 17 on customer churn!)\n",
    "\n",
    "**Your task:** Watch [this short advertisement](https://www.youtube.com/watch?v=MjTZM7VQDzQ) for CBE, a [product](https://www.capturehighered.com/cbe-marketing-automation/) that helps universities \"engage with prospective students\" using a \"behavioral intelligence platform\". Then, in [today's Google Doc](https://docs.google.com/document/d/1Jgg8tKu5gloMtI3bUwyjGX-ugHqnIgnclH15Kdojdpg/edit?usp=sharing), answer at least **one** of the following questions:\n",
    "\n",
    "1. What sort of information could CBE capture that might be useful to feed into an ML algorithm like this?\n",
    "2. What are some possible ethical or fairness concerns about using this information?\n",
    "3. What sort of information about a student _does_ seem reasonable to use as part of the university admissions process?\n",
    "4. What if we're using the same information but having humans make the decisions, instead of ML? Does that change anything?\n",
    "\n",
    "BTW there are clearly some serious _privacy_ issues here, but let's try focus on the ethics/fairness issues.\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) Some notes, for those who are interested:\n",
    "\n",
    "- After making predictions for each student, they may try to optimize for a balanced class, e.g. a mix of \"asset types\" to balance out risks. This is reminiscent of [portfolio optimization](https://en.wikipedia.org/wiki/Portfolio_optimization) in finance.\n",
    "- I suspect the situation is a bit better in Canada where most universities, and all top universities, are public. In the US private universities and colleges are a huge industry, and there are even a lot of for-profit post-secondary institutions.\n",
    "- I suspect the vast majority of prospective students are unaware that this information is being collected. Worse, some of these trackers might track activity _outside_ of the school website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO ADD\n",
    "\n",
    "https://www.theregister.com/2020/07/01/mit_dataset_removed/\n",
    "\n",
    "# TO ADD\n",
    "\n",
    "show LIME for a convnet from lecture 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and AI safety (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "\n",
    "- Bias: we have similar problems as we had with NLP.\n",
    "- These models are trained from huge datasets, we don't exactly know how they work.\n",
    "- See, e.g. [this paper](https://arxiv.org/pdf/1711.11443.pdf), Figure 8.\n",
    "- Discussion: how could this sort of bias affect peoples' lives negatively?\n",
    "\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does bias come from?\n",
    "\n",
    "- Biases could come from data (if data only has certain groups in certain situations).\n",
    "- Biases could come from labels (always using label of \"ball\" for certain sports).\n",
    "- Biases could come from learning method (model predicts \"basketball\" for black people more often than they appear in training data for basketball images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strange results\n",
    "\n",
    "- See [this 2019 paper](https://arxiv.org/pdf/1809.04729.pdf) (from UBC), Figure 1.\n",
    "- See [this 2018 blog post](https://medium.com/@jrzech/what-are-radiological-deep-learning-models-actually-learning-f97a546c5b98) on diagnosing cardiomegaly (enlarged heart).\n",
    "  - The model learns that the portable scanner is used on sicker patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importances to explain predictions\n",
    "\n",
    "- See the README of the [LIME repo](https://github.com/marcotcr/lime).\n",
    "- It can shade the images based on importance, like the above.\n",
    "- Also works for other data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI safety and adversarial examples\n",
    "\n",
    "- See [this 2015 paper](https://arxiv.org/pdf/1412.6572.pdf), Figure 1, for whitebox software attack.\n",
    "- See [this 2018 paper](https://arxiv.org/pdf/1712.09665.pdf), Figure 1, for whitebox **physical** attack. \n",
    "- If the input is a $1000\\times 1000$ image, the input space is so big.\n",
    "  - it is hard to imagine all the possible perturbations, so some of them work out.\n",
    "\n",
    "#### Fake news and deepfakes\n",
    "\n",
    "- You may have heard of [deepfakes](https://en.wikipedia.org/wiki/Deepfake).\n",
    "- See [this 2020 article](https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them).\n",
    "  - which led me to this [hilarious 2019 deepfake video](https://www.youtube.com/watch?v=4GdWD0yxvqw&feature=emb_logo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environmental impact\n",
    "\n",
    "- Current methods require:\n",
    "  – A lot of data.\n",
    "  – A lot of time to train.\n",
    "  – Many training runs to do hyper-parameter optimization.\n",
    "- See [this 2019 paper](https://arxiv.org/pdf/1906.02243.pdf), Table 1, for the CO$_2$ emissions of training a sophisticated deep learning model.\n",
    "  - Entire training procedure emits 5 times more CO2 than lifetime emission of a car, including making the car\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
