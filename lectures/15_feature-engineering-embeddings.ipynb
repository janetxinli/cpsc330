{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CPSC 330 Lecture 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture plan\n",
    "\n",
    "Feature engineering, embeddings, sparse matrices\n",
    "\n",
    "\n",
    "- T/F questions (15 min)\n",
    "- ~~Intro to NLP (5 min)~~\n",
    "- ~~Word counts, TF-IDF (10 min)~~\n",
    "- Break (5 min)\n",
    "- Sparse matrices in Python (10 min)\n",
    "- ~~Preprocessing (15 min)~~\n",
    "- Word embeddings (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import string \n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dependency: `nltk`. Install with\n",
    "\n",
    "`pip install nltk`\n",
    "\n",
    "or\n",
    "\n",
    "`conda install nltk`\n",
    "\n",
    "You may need to install some \"nltk data\" afterwards. If so, nltk itself will give you instruction for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dependency: `gensim`. Install with:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "or \n",
    "\n",
    "`conda install -c conda-forge gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions (15 min)\n",
    "\n",
    "- Removing features generally decreases your training score.\n",
    "- Removing features generally increases your validation score.\n",
    "- Removing a feature can change the feature importances of other features, including the _order_ of those importances.\n",
    "- sklearn's `Lasso` selects features but does not make predictions.\n",
    "\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to NLP (5 min)\n",
    "\n",
    "- Natural Language Processing (NLP) involves extracting information from human language.\n",
    "- There are many possible NLP tasks for many purposes. Here are just a few examples:\n",
    "  - translation\n",
    "  - summarization\n",
    "  - sentiment analysis\n",
    "  - relationship extraction\n",
    "  - question answering / chatbots\n",
    "- NLP is very difficult! Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: Lexical ambiguity\n",
    "\n",
    "\n",
    "<img src=\"img/lexical_ambiguity.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: Part-of-speech ambiguity\n",
    "\n",
    "<center><img src=\"img/pos_ambiguity.png\" width=\"800\" height=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: Referential ambiguity\n",
    "\n",
    "\n",
    "<img src=\"img/referential_ambiguity.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In short, you could do an entire course (or an [entire degree](https://masterdatascience.ubc.ca/programs/computational-linguistics)!) on NLP.\n",
    "- In this class we'll focus on **turning text into numeric features**.\n",
    "- We'll use the [IMDB movie review dataset](https://www.kaggle.com/utathya/imdb-review-dataset) from Kaggle, also used in Lecture 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>This movie was a dismal attempt at recreating ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9940</th>\n",
       "      <td>These days, Ridley Scott is one of the top dir...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45421</th>\n",
       "      <td>on the contrary to the person listed above me ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42236</th>\n",
       "      <td>This is one of those movies that you wish you ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15382</th>\n",
       "      <td>It's hard for me to explain this show to my gr...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "6105   This movie was a dismal attempt at recreating ...   neg\n",
       "9940   These days, Ridley Scott is one of the top dir...   neg\n",
       "45421  on the contrary to the person listed above me ...   pos\n",
       "42236  This is one of those movies that you wish you ...   pos\n",
       "15382  It's hard for me to explain this show to my gr...   pos"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('data/imdb_master.csv', index_col=0, encoding=\"ISO-8859-1\")\n",
    "imdb_df = imdb_df[imdb_df['label'].str.startswith(('pos','neg'))].drop(columns=['file', 'type'])\n",
    "df_train, df_test = train_test_split(imdb_df, random_state=123)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"These days, Ridley Scott is one of the top directors and producers and can command huge sums to helm movies--especially since he has films like ALIEN, GLADIATOR and BLADE RUNNER to his credit. So from this partial list of his credits, it's obvious he's an amazing talent. However, if you watch this very early effort that he made while in film school, you'd probably have a hard time telling that he was destined for greatness. That's because although it has some nice camera-work and style, the film is hopelessly dull and uninvolving. However, considering that it wasn't meant for general release and it was only a training ground, then I am disposed to looking at it charitably--hence the score of 4.<br /><br />By the way, this film is part of the CINEMA 16: European Shorts DVD. On this DVD are 16 shorts. Most aren't great, though because it contains THE MAN WITHOUT A HEAD, COPY SHOP, RABBIT and WASP, it's an amazing DVD for lovers of short films and well worth buying.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1][\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text data\n",
    "\n",
    "- How do we feed it into ML algorithms?\n",
    "- How do we represent the meaning of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts and TF-IDF (10 min)\n",
    "\n",
    "We've seen `CountVectorizer` in Lecture 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.fit(df_train[\"review\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = countvec.transform(df_train[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "(Note: `??` or shift-tab in Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Parameters\u001b[0m\n",
       "\u001b[0;34m        ----------\u001b[0m\n",
       "\u001b[0;34m        raw_documents : iterable\u001b[0m\n",
       "\u001b[0;34m            An iterable which yields either str, unicode or file objects.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Returns\u001b[0m\n",
       "\u001b[0;34m        -------\u001b[0m\n",
       "\u001b[0;34m        self\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??CountVectorizer.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X_train_counts[0].toarray().flatten(), log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a few minutes we'll talk about many of these hyperparameters.\n",
    "- The ones that most simply control the number of features are `max_features`, `min_df`, and `max_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 91308)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 32743)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(min_df=5).fit_transform(df_train[\"review\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 27650)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(min_df=5, max_df=100).fit_transform(df_train[\"review\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(min_df=5, max_df=100, max_features=1000).fit_transform(df_train[\"review\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(vocabulary=[\"good\", \"bad\", \"silly\", \"horrible\"]).fit_transform(df_train[\"review\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Intuition: normalize word count by the frequency of the word in the entire dataset.\n",
    "- If \"earthshattering\" appears 10 times, that is more meaningful than if \"movie\" appears 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape is the same as what we get from `CountVectorizer`, but the counts are normalized - we won't go into the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf = TfidfVectorizer(max_features=1000).fit_transform(df_train[\"review\"])\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X_train_tfidf[0].toarray().flatten(), log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a very simple case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "            \"This is the first document, the FIRST\",\n",
    "            \"This is the second document\",\n",
    "            \"This is the third document\",\n",
    "            \"This is the fourth document\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    document  first  fourth  is  second  the  third  this\n",
       "D1         1      2       0   1       0    2      0     1\n",
       "D2         1      0       0   1       1    1      0     1\n",
       "D3         1      0       0   1       0    1      1     1\n",
       "D4         1      0       1   1       0    1      0     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "header = vectorizer.get_feature_names()\n",
    "labels = ['D1', 'D2', 'D3', 'D4']\n",
    "df = pd.DataFrame(X.toarray(), columns = header, index = labels)  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.214725</td>\n",
       "      <td>0.822953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.429451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.691835</td>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.691835</td>\n",
       "      <td>0.361028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D4</th>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.691835</td>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    document     first    fourth        is    second       the     third  \\\n",
       "D1  0.214725  0.822953  0.000000  0.214725  0.000000  0.429451  0.000000   \n",
       "D2  0.361028  0.000000  0.000000  0.361028  0.691835  0.361028  0.000000   \n",
       "D3  0.361028  0.000000  0.000000  0.361028  0.000000  0.361028  0.691835   \n",
       "D4  0.361028  0.000000  0.691835  0.361028  0.000000  0.361028  0.000000   \n",
       "\n",
       "        this  \n",
       "D1  0.214725  \n",
       "D2  0.361028  \n",
       "D3  0.361028  \n",
       "D4  0.361028  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "header = vectorizer.get_feature_names()\n",
    "labels = ['D1', 'D2', 'D3', 'D4']\n",
    "df = pd.DataFrame(X.toarray(), columns = header, index = labels)  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is also [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) which takes the word counts from `CountVectorizer` and transforms them. The output should be the same as if you used `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse matrices in Python (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had this code earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we need `.toarray()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **sparse matrix**. Why? Look at the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 91308)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3424050000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot! How many are nonzero though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5114043"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014935655145222762"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_nz = X_train_counts.nnz / np.prod(X_train_counts.shape)\n",
    "frac_nz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This happens because most words do not appear in a given document. \n",
    "- We get massive computational savings if we **only store the nonzero elements**. \n",
    "- There is a bit of overhead, because we also need to **store the locations**:\n",
    "  - e.g. \"location (5,192): 3\".\n",
    "  - However, if the fraction of nonzero is small, this is a huge win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the nonzero elements in the first review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1893)\t1\n",
      "  (0, 1894)\t1\n",
      "  (0, 4036)\t2\n",
      "  (0, 4378)\t1\n",
      "  (0, 5338)\t1\n",
      "  (0, 5509)\t1\n",
      "  (0, 5868)\t2\n",
      "  (0, 6013)\t1\n",
      "  (0, 7736)\t2\n",
      "  (0, 7843)\t1\n",
      "  (0, 8109)\t1\n",
      "  (0, 8806)\t1\n",
      "  (0, 10625)\t1\n",
      "  (0, 10826)\t6\n",
      "  (0, 12242)\t2\n",
      "  (0, 13705)\t1\n",
      "  (0, 14076)\t1\n",
      "  (0, 15561)\t1\n",
      "  (0, 18502)\t1\n",
      "  (0, 19264)\t3\n",
      "  (0, 19379)\t1\n",
      "  (0, 20503)\t1\n",
      "  (0, 20762)\t1\n",
      "  (0, 21067)\t1\n",
      "  (0, 22422)\t2\n",
      "  :\t:\n",
      "  (0, 63802)\t2\n",
      "  (0, 66090)\t1\n",
      "  (0, 66751)\t1\n",
      "  (0, 66890)\t1\n",
      "  (0, 67724)\t1\n",
      "  (0, 69096)\t1\n",
      "  (0, 70346)\t1\n",
      "  (0, 73319)\t1\n",
      "  (0, 74862)\t1\n",
      "  (0, 76703)\t1\n",
      "  (0, 80828)\t1\n",
      "  (0, 80846)\t12\n",
      "  (0, 81118)\t4\n",
      "  (0, 81556)\t1\n",
      "  (0, 81794)\t2\n",
      "  (0, 83065)\t1\n",
      "  (0, 83651)\t1\n",
      "  (0, 86703)\t1\n",
      "  (0, 86710)\t1\n",
      "  (0, 87889)\t3\n",
      "  (0, 88008)\t7\n",
      "  (0, 88172)\t3\n",
      "  (0, 88502)\t2\n",
      "  (0, 88885)\t1\n",
      "  (0, 89196)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BTW, have you noticed that with `OneHotEncoder` we've been setting `sparse=False`. \n",
    "- This is to get it to return a regular numpy array instead of a sparse array, so we didn't have to deal with these.\n",
    "- If there are a huge number of categories, it may be beneficial to keep them as sparse.\n",
    "- For smaller number of categories, it doesn't matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with scipy.sparse matrices\n",
    "\n",
    "- We won't go into implementation details here, but there are some \"gotchas\" with `scipy.sparse`.\n",
    "- For example, with a regular numpy array, `x[i,j]` and `x[i][j]` are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19861089003333154"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19861089003333154"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19861089003333154"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[1])[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because `x[1]` returns the first row, and then the `[2]` indexes into that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65399277, 0.84521827, 0.19861089, 0.00167515, 0.64181605,\n",
       "       0.25209052, 0.01963685, 0.51772779, 0.19828813, 0.44265865])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19861089003333154"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_1 = x[1]\n",
    "row_1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with `scipy.sparse` matrices, things are a bit different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sparse = scipy.sparse.csr_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18289061542864404"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sparse[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "row index (2) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e1ac3b8942d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_sparse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mM\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: row index (2) out of range"
     ]
    }
   ],
   "source": [
    "x_sparse[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1_sparse = x_sparse[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_1_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sparse matrix returns a different shape, leaving in the first dimension.\n",
    "- This can be annoying and is something to watch out for.\n",
    "- In general, I suggest using the `x[1,2]` notation when possible because chaining the `[]` can be problematic in several places (e.g., also pandas).\n",
    "- However, this is only for numpy, not, say, a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6], [7, 9]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[1, 2, 3], [4, 5, 6], [7, 9]]\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0317db6cb532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "lst[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparse matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find the highest word count. But first, review the `axis` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 4, 8, 0, 0],\n",
       "       [6, 2, 5, 9, 7],\n",
       "       [1, 2, 3, 4, 0],\n",
       "       [0, 1, 6, 3, 6]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(10, size=(4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  9, 22, 16, 13])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 29, 10, 16])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<37500x1 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 37500 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document with the highest word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26017"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(X_train_counts.max(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regular numpy functions work on sparse matrices, although they might be fast/slow depending. \n",
    "- You definitely do not want to iterate with loops - make sure you use builtin functions. \n",
    "- There are some details here, in that a sparse matrix max be stored row-by-row or column-by-column, and this affects speed.\n",
    "  - This is beyond the scope of the course, but something to look into if your code is too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back to the various hyperparameters of `CountVectorizer`, such as:\n",
    "  - `strip_accents=None`\n",
    "  - `lowercase=True`\n",
    "  - `preprocessor=None`\n",
    "  - `tokenizer=None`\n",
    "  - `stop_words=None`\n",
    "  - `token_pattern='(?u)\\b\\w\\w+\\b'`\n",
    "  - `ngram_range=(1, 1)`\n",
    "  - `analyzer='word'`\n",
    "- What do these all mean?\n",
    "- They have to do with how the text is **preprocessed** before the words are counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why preprocessing?\n",
    "\n",
    "- We need to split things apart:\n",
    "    - Segment sentences\n",
    "    - Tokenize words\n",
    "- We want equivalent things to match with each other\n",
    "    * USA &rarr; U.S.A.\n",
    "    * You're &rarr; you are\n",
    "    * Hello &rarr; hello\n",
    "    * Vancouver's &rarr; Vancouver\n",
    "    * computers &rarr; computer \n",
    "    * rising &rarr; rise, rose, rises\n",
    "- We might want to get rid of commonly occurring words\n",
    "    * to, for, the, a, an, of, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Usual preprocessing pipeline\n",
    "\n",
    "- Preprocessing is task dependent but given raw text data the usual pipeline is :\n",
    "    - Sentence segmentation\n",
    "    - Tokenization \n",
    "    - Stopword removal\n",
    "    - Stemming or lemmatization (in some cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tools for preprocessing \n",
    "\n",
    "- Many available tools\n",
    "- spaCy is a good one to know about\n",
    "- We will be using [Natural Language Processing Toolkit (nltk)](https://www.nltk.org/)\n",
    "    - Platform to build NLP programs with Python\n",
    "    - Provides access to a number of NLP preprocessing tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sentence segmentation (finding sentence boundaries)\n",
    "\n",
    "- ! and ? are relatively ambiguous.\n",
    "- Period (.) is quite ambiguous.\n",
    "    - Sentence boundary?\n",
    "    - Abbreviations like Dr., U.S., Inc.  \n",
    "    - Numbers like 60.44%, 0.98\n",
    "- Build a binary classifier to decide whether a period is a sentence boundary marker or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These days, Ridley Scott is one of the top directors and producers and can command huge sums to helm movies--especially since he has films like ALIEN, GLADIATOR and BLADE RUNNER to his credit. So from this partial list of his credits, it's obvious he's an amazing talent. However, if you watch this very early effort that he made while in film school, you'd probably have a hard time telling that he was destined for greatness. That's because although it has some nice camera-work and style, the film is hopelessly dull and uninvolving. However, considering that it wasn't meant for general release and it was only a training ground, then I am disposed to looking at it charitably--hence the score of 4.\n",
      "\n",
      "By the way, this film is part of the CINEMA 16: European Shorts DVD. On this DVD are 16 shorts. Most aren't great, though because it contains THE MAN WITHOUT A HEAD, COPY SHOP, RABBIT and WASP, it's an amazing DVD for lovers of short films and well worth buying.\n"
     ]
    }
   ],
   "source": [
    "review = df_train.iloc[1][\"review\"].replace(\"<br />\", \"\\n\")\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These days, Ridley Scott is one of the top directors and producers and can command huge sums to helm movies--especially since he has films like ALIEN, GLADIATOR and BLADE RUNNER to his credit.',\n",
       " \"So from this partial list of his credits, it's obvious he's an amazing talent.\",\n",
       " \"However, if you watch this very early effort that he made while in film school, you'd probably have a hard time telling that he was destined for greatness.\",\n",
       " \"That's because although it has some nice camera-work and style, the film is hopelessly dull and uninvolving.\",\n",
       " \"However, considering that it wasn't meant for general release and it was only a training ground, then I am disposed to looking at it charitably--hence the score of 4.\",\n",
       " 'By the way, this film is part of the CINEMA 16: European Shorts DVD.',\n",
       " 'On this DVD are 16 shorts.',\n",
       " \"Most aren't great, though because it contains THE MAN WITHOUT A HEAD, COPY SHOP, RABBIT and WASP, it's an amazing DVD for lovers of short films and well worth buying.\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sentences = sent_tokenize(review)\n",
    "review_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tokenization (finding word boundaries)\n",
    "\n",
    "- Is presence of whitespace is not a sufficient condition for a word boundary? \n",
    "- Should we split on the special characters then? \n",
    "    - What about words like _grown-ups_ then?\n",
    "- This process of identifying word boundaries is referred to as **tokenization**.\n",
    "- It is not as simple and is done by training ML models.\n",
    "- It gets even messier in other languages like Chinese (no spaces) and German (very long words).\n",
    "    * rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz \n",
    "    * (the law for the delegation of monitoring beef labeling)\n",
    "- We can use `nltk.word_tokenize`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These days, Ridley Scott is one of the top directors and producers and can command huge sums to helm'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These',\n",
       " 'days',\n",
       " ',',\n",
       " 'Ridley',\n",
       " 'Scott',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'top',\n",
       " 'directors',\n",
       " 'and',\n",
       " 'producers',\n",
       " 'and',\n",
       " 'can',\n",
       " 'command',\n",
       " 'huge',\n",
       " 'sums',\n",
       " 'to',\n",
       " 'helm']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(review)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Punctuation and stoplist\n",
    "\n",
    "- The most frequently occurring words are actually the most frequently occurring words in English\n",
    "    - Example: _the_, _is_, _a_, and punctuation\n",
    "- Probably not very informative for certain tasks \n",
    "    - Let's use `nltk.stopwords`.  \n",
    "    - Add punctuations to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needn't, nor, them, than, aren, during, you've, she's, being, yours, had, wouldn't, each, about, where, an, they, him, before, couldn't, don't, me, which, himself, doing, or, out, o, herself, did, that, under, there, whom, into, was, have, mightn't, ve, as, again, having, ma, you, weren, while, you'll, wouldn, but, in, you're, who, didn, she, has, her, shan't, wasn, you'd, after, from, down, he, yourselves, shouldn, any, theirs, until, hasn, haven, how, will, just, other, because, this, why, aren't, for, been, hasn't, haven't, mightn, such, ours, further, that'll, re, themselves, now, all, do, to, ll, am, own, over, hadn't, when, my, it's, a, up, doesn't, should've, on, won't, can, more, s, y, not, don, shan, of, mustn, shouldn't, their, above, it, does, hadn, were, then, we, some, our, too, won, yourself, between, wasn't, at, very, these, are, isn't, those, same, couldn, needn, t, your, here, and, off, didn't, is, hers, m, if, his, what, ain, by, once, few, myself, through, both, so, be, with, i, weren't, doesn, against, should, d, only, mustn't, the, no, most, itself, isn, its, below, ourselves, !, \", #, $, %, &, ', (, ), *, +, ,, -, ., /, :, ;, <, =, >, ?, @, [, \\, ], ^, _, `, {, |, }, ~\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words += list(string.punctuation)\n",
    "print(', '.join(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be set with the `stop_words` parameter with `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stemming\n",
    "\n",
    "- Stemming is a crude chopping of affixes \n",
    "    * _automates, automatic, automation_ all reduced to _automat_.\n",
    "- Beware that it can be aggressive sometimes.\n",
    "- This is something `CountVectorizer` does **not** do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(countvec.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wait',\n",
       " 'waite',\n",
       " 'waited',\n",
       " 'waiter',\n",
       " 'waiters',\n",
       " 'waites',\n",
       " 'waiting',\n",
       " 'waitress',\n",
       " 'waitresses',\n",
       " 'waitressing',\n",
       " 'waits',\n",
       " 'waitâ'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v for v in vocab if v.startswith('wait')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try stemming before finding the most frequently occurring words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "vocab_stemmed = [ps.stem(v) for v in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wait', 'waiter', 'waitress', 'waitâ'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v for v in vocab_stemmed if v.startswith('wait')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lemmatization\n",
    "\n",
    "- In some tasks, it makes sense to reduce all forms of a word to its base form (e.g., *lemma* or *stem*)\n",
    "    * am, are, is &rarr; be\n",
    "    * went, going, goes, gone &rarr; go\n",
    "    * We can address this by using [nltk.stem.wordnet](https://www.nltk.org/_modules/nltk/stem/wordnet.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went --> go\n",
      "being --> be\n",
      "gone --> go\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "verbs = ['went', 'being', 'gone']\n",
    "nouns = ['wolves', 'assessment']\n",
    "for word in verbs: \n",
    "    print('%s --> %s' %(word, wnl.lemmatize(word,'v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### $n$-grams \n",
    "\n",
    "- A contiguous sequence of _n_ items (characters, tokens) in text\n",
    "- bigrams: a contiguous sequence of two items\n",
    "- trigrams: a contiguous sequence of three items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec_bigram = CountVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "countvec_bigram.fit(df_train[:1000][\"review\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 826,\n",
       " 'movie': 516,\n",
       " 'was': 914,\n",
       " 'attempt': 79,\n",
       " 'at': 75,\n",
       " 'time': 840,\n",
       " 'in': 384,\n",
       " 'english': 226,\n",
       " 'history': 361,\n",
       " 'the': 763,\n",
       " 'film': 268,\n",
       " 'version': 897,\n",
       " 'of': 555,\n",
       " 'war': 913,\n",
       " 'is': 402,\n",
       " 'but': 128,\n",
       " 'overall': 595,\n",
       " 'way': 926,\n",
       " 'off': 570,\n",
       " 'that': 753,\n",
       " 'started': 726,\n",
       " 'over': 593,\n",
       " 'no': 542,\n",
       " 'both': 114,\n",
       " 'were': 934,\n",
       " 'never': 537,\n",
       " 'did': 190,\n",
       " 'he': 346,\n",
       " 'ever': 241,\n",
       " 'save': 655,\n",
       " 'day': 182,\n",
       " 'not': 545,\n",
       " 'br': 117,\n",
       " 'as': 69,\n",
       " 'another': 57,\n",
       " 'has': 338,\n",
       " 'certainly': 149,\n",
       " 'one': 579,\n",
       " 'who': 949,\n",
       " 'to': 843,\n",
       " 'be': 88,\n",
       " 'from': 297,\n",
       " 'house': 370,\n",
       " 'and': 37,\n",
       " 'decent': 186,\n",
       " 'hard': 336,\n",
       " 'it': 417,\n",
       " 'didn': 192,\n",
       " 'turn': 877,\n",
       " 'out': 590,\n",
       " 'so': 707,\n",
       " 'great': 323,\n",
       " 'definitely': 187,\n",
       " 'could': 170,\n",
       " 'have': 341,\n",
       " 'been': 95,\n",
       " 'better': 103,\n",
       " 'this movie': 829,\n",
       " 'movie was': 522,\n",
       " 'the film': 777,\n",
       " 'version of': 898,\n",
       " 'in the': 391,\n",
       " 'but the': 133,\n",
       " 'of this': 567,\n",
       " 'this film': 827,\n",
       " 'that the': 757,\n",
       " 'at the': 78,\n",
       " 'did not': 191,\n",
       " 'br br': 118,\n",
       " 'one of': 580,\n",
       " 'of the': 563,\n",
       " 'to be': 844,\n",
       " 'from the': 298,\n",
       " 'and it': 44,\n",
       " 'could have': 172,\n",
       " 'have been': 342,\n",
       " 'these': 815,\n",
       " 'days': 183,\n",
       " 'top': 866,\n",
       " 'can': 142,\n",
       " 'movies': 523,\n",
       " 'especially': 234,\n",
       " 'since': 703,\n",
       " 'films': 275,\n",
       " 'like': 464,\n",
       " 'his': 360,\n",
       " 'obvious': 554,\n",
       " 'an': 36,\n",
       " 'amazing': 33,\n",
       " 'talent': 745,\n",
       " 'however': 372,\n",
       " 'if': 378,\n",
       " 'you': 989,\n",
       " 'watch': 920,\n",
       " 'very': 899,\n",
       " 'early': 216,\n",
       " 'effort': 220,\n",
       " 'made': 487,\n",
       " 'while': 947,\n",
       " 'school': 663,\n",
       " 'probably': 626,\n",
       " 'for': 284,\n",
       " 'because': 91,\n",
       " 'although': 30,\n",
       " 'some': 710,\n",
       " 'nice': 540,\n",
       " 'camera': 141,\n",
       " 'work': 969,\n",
       " 'style': 737,\n",
       " 'wasn': 918,\n",
       " 'only': 581,\n",
       " 'then': 809,\n",
       " 'am': 32,\n",
       " 'looking': 479,\n",
       " 'score': 664,\n",
       " 'by': 136,\n",
       " 'part': 597,\n",
       " 'cinema': 157,\n",
       " 'dvd': 213,\n",
       " 'on': 575,\n",
       " 'are': 64,\n",
       " 'most': 512,\n",
       " 'though': 834,\n",
       " 'man': 493,\n",
       " 'without': 963,\n",
       " 'head': 350,\n",
       " 'short': 690,\n",
       " 'well': 931,\n",
       " 'worth': 974,\n",
       " 'is one': 411,\n",
       " 'he has': 347,\n",
       " 'to his': 850,\n",
       " 'of his': 560,\n",
       " 'if you': 382,\n",
       " 'watch this': 922,\n",
       " 'that he': 754,\n",
       " 'he was': 349,\n",
       " 'it has': 421,\n",
       " 'film is': 270,\n",
       " 'that it': 756,\n",
       " 'it was': 428,\n",
       " 'by the': 137,\n",
       " 'the way': 802,\n",
       " 'part of': 598,\n",
       " 'on this': 577,\n",
       " 'because it': 92,\n",
       " 'the man': 782,\n",
       " 'person': 607,\n",
       " 'above': 6,\n",
       " 'me': 498,\n",
       " 'felt': 265,\n",
       " 'really': 638,\n",
       " 'funny': 301,\n",
       " 'scenes': 662,\n",
       " 'there': 810,\n",
       " 'lot': 482,\n",
       " 'up': 888,\n",
       " 'don': 206,\n",
       " 'want': 907,\n",
       " 'give': 312,\n",
       " 'plot': 618,\n",
       " 'storyline': 733,\n",
       " 'away': 81,\n",
       " 'people': 601,\n",
       " 'watched': 923,\n",
       " 'yet': 988,\n",
       " 'will': 955,\n",
       " 'say': 658,\n",
       " 'does': 202,\n",
       " 'role': 647,\n",
       " 'such': 738,\n",
       " 'past': 600,\n",
       " 'example': 248,\n",
       " 'little': 470,\n",
       " 'given': 313,\n",
       " 'him': 358,\n",
       " 'john': 435,\n",
       " 'less': 461,\n",
       " 'had': 328,\n",
       " 'almost': 25,\n",
       " 'all': 22,\n",
       " 'after': 17,\n",
       " 'even': 236,\n",
       " 'three': 836,\n",
       " 'they': 816,\n",
       " 'pretty': 625,\n",
       " 'good': 320,\n",
       " 'job': 434,\n",
       " 'acting': 10,\n",
       " 'would': 975,\n",
       " 'liked': 467,\n",
       " 'or': 584,\n",
       " 'must': 531,\n",
       " 'on the': 576,\n",
       " 'to the': 857,\n",
       " 'that this': 760,\n",
       " 'there is': 812,\n",
       " 'lot of': 483,\n",
       " 'want to': 908,\n",
       " 'the plot': 790,\n",
       " 'and the': 48,\n",
       " 'say that': 659,\n",
       " 'does not': 203,\n",
       " 'such as': 739,\n",
       " 'he is': 348,\n",
       " 'is no': 409,\n",
       " 'in this': 393,\n",
       " 'even though': 239,\n",
       " 'who is': 951,\n",
       " 'is very': 415,\n",
       " 'very good': 900,\n",
       " 'movie is': 520,\n",
       " 'those': 833,\n",
       " 'wish': 957,\n",
       " 'seen': 677,\n",
       " 'before': 96,\n",
       " 'see': 668,\n",
       " 'again': 19,\n",
       " 'first': 280,\n",
       " 'still': 728,\n",
       " 'many': 494,\n",
       " 'far': 260,\n",
       " 'best': 102,\n",
       " 'works': 970,\n",
       " 'more': 510,\n",
       " 'story': 731,\n",
       " 'too': 863,\n",
       " 'last': 452,\n",
       " 'enjoy': 227,\n",
       " 'this is': 828,\n",
       " 'of those': 568,\n",
       " 'that you': 762,\n",
       " 'see it': 669,\n",
       " 'for the': 289,\n",
       " 'the first': 778,\n",
       " 'the best': 767,\n",
       " 'the story': 798,\n",
       " 'you want': 996,\n",
       " 'it to': 427,\n",
       " 'show': 696,\n",
       " 'my': 532,\n",
       " 'friends': 296,\n",
       " 'back': 83,\n",
       " 'which': 945,\n",
       " 'call': 138,\n",
       " 'laugh': 455,\n",
       " 'look': 477,\n",
       " 'series': 682,\n",
       " 'us': 891,\n",
       " 'remember': 642,\n",
       " 'with': 958,\n",
       " 'our': 589,\n",
       " 'other': 586,\n",
       " 'where': 943,\n",
       " 'kids': 441,\n",
       " 'tv': 880,\n",
       " 'today': 860,\n",
       " 'characters': 153,\n",
       " 'do': 199,\n",
       " 'talking': 747,\n",
       " 'lives': 472,\n",
       " 'under': 884,\n",
       " 'real': 636,\n",
       " 'for me': 287,\n",
       " 'this show': 831,\n",
       " 'it would': 429,\n",
       " 'would be': 976,\n",
       " 'to see': 855,\n",
       " 'that there': 758,\n",
       " 'bad': 85,\n",
       " 'how': 371,\n",
       " 'got': 322,\n",
       " 'what': 936,\n",
       " 'know': 448,\n",
       " 'book': 112,\n",
       " 'stories': 730,\n",
       " 'genre': 305,\n",
       " 'plays': 616,\n",
       " 'her': 353,\n",
       " 'father': 261,\n",
       " 'room': 649,\n",
       " 'several': 686,\n",
       " 'said': 653,\n",
       " 'starts': 727,\n",
       " 'why': 953,\n",
       " 'much': 525,\n",
       " 'into': 399,\n",
       " 'something': 713,\n",
       " 'thought': 835,\n",
       " 'death': 185,\n",
       " 'king': 447,\n",
       " 'we': 928,\n",
       " 're': 634,\n",
       " 'about': 3,\n",
       " 'ending': 224,\n",
       " 'same': 654,\n",
       " 'fact': 255,\n",
       " 'whole': 952,\n",
       " 'feeling': 264,\n",
       " 'every': 243,\n",
       " 'dead': 184,\n",
       " 'done': 208,\n",
       " 'anyway': 62,\n",
       " 'star': 723,\n",
       " 'least': 457,\n",
       " 'trying': 875,\n",
       " 'rest': 643,\n",
       " 'left': 460,\n",
       " 'the book': 768,\n",
       " 'even if': 237,\n",
       " 'if it': 379,\n",
       " 'and is': 43,\n",
       " 'so much': 709,\n",
       " 'would have': 977,\n",
       " 'it not': 425,\n",
       " 'not to': 549,\n",
       " 'the ending': 774,\n",
       " 'the same': 793,\n",
       " 'as the': 73,\n",
       " 'in fact': 387,\n",
       " 'the whole': 803,\n",
       " 'too much': 864,\n",
       " 'has to': 340,\n",
       " 'at least': 77,\n",
       " 'trying to': 876,\n",
       " 'to do': 845,\n",
       " 'the rest': 792,\n",
       " 'rest of': 644,\n",
       " 'the movie': 784,\n",
       " 'go': 315,\n",
       " 'believe': 101,\n",
       " 'based': 86,\n",
       " 'true': 871,\n",
       " 'seems': 675,\n",
       " 'written': 982,\n",
       " 'anyone': 60,\n",
       " 'old': 574,\n",
       " 'now': 552,\n",
       " 'rather': 632,\n",
       " 'wife': 954,\n",
       " 'enough': 229,\n",
       " 'make': 490,\n",
       " 'being': 100,\n",
       " 'young': 998,\n",
       " 'woman': 964,\n",
       " 'years': 986,\n",
       " 'guess': 325,\n",
       " 'doesn': 204,\n",
       " 'writer': 980,\n",
       " 'through': 837,\n",
       " 'across': 8,\n",
       " 'black': 110,\n",
       " 'white': 948,\n",
       " 'she': 688,\n",
       " 'when': 939,\n",
       " 'ago': 21,\n",
       " 'them': 807,\n",
       " 'called': 139,\n",
       " 'later': 454,\n",
       " 'either': 221,\n",
       " 'put': 629,\n",
       " 'also': 29,\n",
       " 'someone': 712,\n",
       " 'come': 164,\n",
       " 'gives': 314,\n",
       " 'eyes': 253,\n",
       " 'become': 93,\n",
       " 'think': 823,\n",
       " 'simple': 701,\n",
       " 'idea': 376,\n",
       " 'sure': 742,\n",
       " 'tries': 869,\n",
       " 'second': 667,\n",
       " 'becomes': 94,\n",
       " 'number': 553,\n",
       " 'two': 881,\n",
       " 'against': 20,\n",
       " 'its': 430,\n",
       " 'll': 474,\n",
       " 'wants': 911,\n",
       " 'thing': 821,\n",
       " 'place': 611,\n",
       " 'performance': 604,\n",
       " 'guy': 326,\n",
       " 'everyone': 244,\n",
       " 'else': 222,\n",
       " 'things': 822,\n",
       " 'original': 585,\n",
       " 'told': 862,\n",
       " 'murder': 528,\n",
       " 'long': 476,\n",
       " 'down': 209,\n",
       " 'face': 254,\n",
       " 'voice': 906,\n",
       " 'except': 250,\n",
       " 'events': 240,\n",
       " 'nothing': 550,\n",
       " 'their': 806,\n",
       " 'than': 751,\n",
       " 'life': 463,\n",
       " 'mean': 499,\n",
       " 'character': 152,\n",
       " 'stuff': 735,\n",
       " 'used': 893,\n",
       " 'let': 462,\n",
       " 'maybe': 497,\n",
       " 'here': 354,\n",
       " 'get': 307,\n",
       " 'problem': 627,\n",
       " 'behind': 99,\n",
       " 'based on': 87,\n",
       " 'to have': 849,\n",
       " 'enough to': 230,\n",
       " 'to make': 852,\n",
       " 'some of': 711,\n",
       " 'that they': 759,\n",
       " 'they were': 820,\n",
       " 'and her': 40,\n",
       " 'as well': 74,\n",
       " 'it is': 423,\n",
       " 'is in': 407,\n",
       " 'but it': 130,\n",
       " 'over the': 594,\n",
       " 'have to': 344,\n",
       " 'no one': 543,\n",
       " 'movie that': 521,\n",
       " 'but not': 131,\n",
       " 'with his': 960,\n",
       " 'with the': 961,\n",
       " 'all the': 24,\n",
       " 'who has': 950,\n",
       " 'and was': 53,\n",
       " 'wants to': 912,\n",
       " 'what the': 938,\n",
       " 'seems to': 676,\n",
       " 'so many': 708,\n",
       " 'that was': 761,\n",
       " 'it all': 418,\n",
       " 'people who': 602,\n",
       " 'to get': 847,\n",
       " 'and you': 54,\n",
       " 'along': 26,\n",
       " 'low': 486,\n",
       " 'isn': 416,\n",
       " 'love': 484,\n",
       " 'sex': 687,\n",
       " 'everything': 245,\n",
       " 'hollywood': 363,\n",
       " 'kind': 445,\n",
       " 'late': 453,\n",
       " 'night': 541,\n",
       " 'local': 475,\n",
       " 'just': 437,\n",
       " 'take': 743,\n",
       " 'despite': 188,\n",
       " 'lines': 469,\n",
       " 'times': 841,\n",
       " 'different': 194,\n",
       " 'worse': 972,\n",
       " 'your': 999,\n",
       " 'along with': 27,\n",
       " 'for it': 286,\n",
       " 'is not': 410,\n",
       " 'is the': 414,\n",
       " 'kind of': 446,\n",
       " 'that is': 755,\n",
       " 'is just': 408,\n",
       " 'to take': 856,\n",
       " 'have the': 343,\n",
       " 'movie and': 517,\n",
       " 'has been': 339,\n",
       " 'this one': 830,\n",
       " 'feel': 263,\n",
       " 'point': 619,\n",
       " 'view': 903,\n",
       " 'mostly': 514,\n",
       " 'heart': 351,\n",
       " 'right': 646,\n",
       " 'set': 685,\n",
       " 'perhaps': 606,\n",
       " 'bit': 108,\n",
       " 'read': 635,\n",
       " 'others': 587,\n",
       " 'ends': 225,\n",
       " 'actually': 16,\n",
       " 'kill': 442,\n",
       " 'end': 223,\n",
       " 'audience': 80,\n",
       " 'girl': 311,\n",
       " 'follow': 282,\n",
       " 'ok': 573,\n",
       " 'killed': 443,\n",
       " 'totally': 867,\n",
       " 'evil': 246,\n",
       " 'any': 58,\n",
       " 'case': 147,\n",
       " 'excellent': 249,\n",
       " 'unfortunately': 886,\n",
       " 'comes': 166,\n",
       " 'supposed': 740,\n",
       " 'might': 502,\n",
       " 'serious': 683,\n",
       " 'jokes': 436,\n",
       " 'highly': 356,\n",
       " 'find': 278,\n",
       " 'scene': 661,\n",
       " 'final': 276,\n",
       " 'may': 496,\n",
       " 'leave': 458,\n",
       " 'seem': 672,\n",
       " 'own': 596,\n",
       " 'opinion': 583,\n",
       " 'hope': 365,\n",
       " 'try': 874,\n",
       " 'watching': 924,\n",
       " 'minutes': 504,\n",
       " 'screen': 665,\n",
       " 'there are': 811,\n",
       " 'the acting': 764,\n",
       " 'acting is': 11,\n",
       " 'very well': 901,\n",
       " 'get the': 308,\n",
       " 'in it': 389,\n",
       " 'more than': 511,\n",
       " 'don know': 207,\n",
       " 'much of': 527,\n",
       " 'is that': 413,\n",
       " 'the fact': 776,\n",
       " 'fact that': 256,\n",
       " 'think that': 824,\n",
       " 'at all': 76,\n",
       " 'the end': 773,\n",
       " 'the audience': 766,\n",
       " 'to kill': 851,\n",
       " 'and his': 41,\n",
       " 'any of': 59,\n",
       " 'hard to': 337,\n",
       " 'supposed to': 741,\n",
       " 'br there': 123,\n",
       " 'of them': 565,\n",
       " 'they re': 819,\n",
       " 'to say': 854,\n",
       " 'the scene': 794,\n",
       " 'where the': 944,\n",
       " 'seem to': 673,\n",
       " 'the time': 799,\n",
       " 'of what': 569,\n",
       " 'what it': 937,\n",
       " 'like this': 466,\n",
       " 'hit': 362,\n",
       " 'high': 355,\n",
       " 'wonderful': 967,\n",
       " 'classic': 161,\n",
       " 'himself': 359,\n",
       " 'looks': 480,\n",
       " 'husband': 375,\n",
       " 'son': 715,\n",
       " 'tell': 749,\n",
       " 'playing': 615,\n",
       " 'goes': 317,\n",
       " 'michael': 501,\n",
       " 'begins': 98,\n",
       " 'sequence': 681,\n",
       " 'among': 35,\n",
       " 'typical': 883,\n",
       " 'turns': 879,\n",
       " 'simply': 702,\n",
       " 'director': 197,\n",
       " 'art': 68,\n",
       " 'involved': 401,\n",
       " 'family': 257,\n",
       " 'turned': 878,\n",
       " 'next': 539,\n",
       " 'word': 968,\n",
       " 'finally': 277,\n",
       " 'gets': 309,\n",
       " 'dialogue': 189,\n",
       " 'already': 28,\n",
       " 'home': 364,\n",
       " 'lost': 481,\n",
       " 'big': 107,\n",
       " 'until': 887,\n",
       " 'slow': 705,\n",
       " 'killer': 444,\n",
       " 'shots': 692,\n",
       " 'new': 538,\n",
       " 'making': 492,\n",
       " 'wanted': 909,\n",
       " 'once': 578,\n",
       " 'strong': 734,\n",
       " 'understand': 885,\n",
       " 'interesting': 398,\n",
       " 'viewer': 904,\n",
       " 'around': 67,\n",
       " 'running': 651,\n",
       " 'need': 536,\n",
       " 'somewhat': 714,\n",
       " 'makes': 491,\n",
       " 'check': 154,\n",
       " 'which is': 946,\n",
       " 'than the': 752,\n",
       " 'br the': 122,\n",
       " 'in which': 394,\n",
       " 'she is': 689,\n",
       " 'as he': 71,\n",
       " 'with her': 959,\n",
       " 'and in': 42,\n",
       " 'the director': 771,\n",
       " 'br in': 120,\n",
       " 'the second': 796,\n",
       " 'up to': 889,\n",
       " 'and that': 47,\n",
       " 'we are': 929,\n",
       " 'when the': 942,\n",
       " 'into the': 400,\n",
       " 'we have': 930,\n",
       " 'wanted to': 910,\n",
       " 'and he': 39,\n",
       " 'up with': 890,\n",
       " 'and they': 51,\n",
       " 'you have': 992,\n",
       " 'in an': 386,\n",
       " 'br it': 121,\n",
       " 'about this': 5,\n",
       " 'but that': 132,\n",
       " 'story is': 732,\n",
       " 'feature': 262,\n",
       " 'friend': 295,\n",
       " 'released': 641,\n",
       " 'american': 34,\n",
       " 'game': 303,\n",
       " 'shows': 698,\n",
       " 'was not': 915,\n",
       " 'not the': 548,\n",
       " 'film to': 273,\n",
       " 'was the': 916,\n",
       " 'can be': 143,\n",
       " 'beginning': 97,\n",
       " 'quite': 631,\n",
       " 'seemed': 674,\n",
       " 'completely': 169,\n",
       " 'picture': 608,\n",
       " 'wouldn': 978,\n",
       " 'god': 316,\n",
       " 'awful': 82,\n",
       " 'each': 214,\n",
       " 'script': 666,\n",
       " 'direction': 196,\n",
       " 'out of': 591,\n",
       " 'the one': 786,\n",
       " 'each other': 215,\n",
       " 'like the': 465,\n",
       " 'when it': 941,\n",
       " 'play': 612,\n",
       " 'cast': 148,\n",
       " 'ridiculous': 645,\n",
       " 'act': 9,\n",
       " 'silly': 700,\n",
       " 'police': 620,\n",
       " 'always': 31,\n",
       " 'help': 352,\n",
       " 'is an': 404,\n",
       " 'most of': 513,\n",
       " 'the cast': 769,\n",
       " 'french': 294,\n",
       " 'brother': 126,\n",
       " 'found': 292,\n",
       " 'action': 12,\n",
       " 'car': 145,\n",
       " 'fight': 267,\n",
       " 'line': 468,\n",
       " 'going': 318,\n",
       " 'to find': 846,\n",
       " 'the script': 795,\n",
       " 'couple': 175,\n",
       " 'title': 842,\n",
       " 'men': 500,\n",
       " 'the last': 780,\n",
       " 'couple of': 176,\n",
       " 'is also': 403,\n",
       " 'course': 177,\n",
       " 'beautiful': 90,\n",
       " 'die': 193,\n",
       " 'fan': 258,\n",
       " 'matter': 495,\n",
       " 'expect': 251,\n",
       " 'recommend': 640,\n",
       " 'is so': 412,\n",
       " 'and even': 38,\n",
       " 'of course': 558,\n",
       " 'you are': 990,\n",
       " 'you will': 997,\n",
       " 'to watch': 859,\n",
       " 'won': 966,\n",
       " 'waste': 919,\n",
       " 'hours': 369,\n",
       " 'actor': 13,\n",
       " 'name': 534,\n",
       " 'cut': 179,\n",
       " 'movie br': 518,\n",
       " 'it br': 420,\n",
       " 'horror': 367,\n",
       " 'happens': 334,\n",
       " 'flick': 281,\n",
       " 'talk': 746,\n",
       " 'video': 902,\n",
       " 'going to': 319,\n",
       " 'they are': 817,\n",
       " 'the good': 779,\n",
       " 'it in': 422,\n",
       " 'boring': 113,\n",
       " 'horrible': 366,\n",
       " 'money': 508,\n",
       " 'it just': 424,\n",
       " 'gave': 304,\n",
       " 'children': 156,\n",
       " 'this was': 832,\n",
       " 'ever seen': 242,\n",
       " 'you can': 991,\n",
       " 'terrible': 750,\n",
       " 'self': 678,\n",
       " 'care': 146,\n",
       " 'oh': 572,\n",
       " 'came': 140,\n",
       " 'moment': 506,\n",
       " 'complete': 168,\n",
       " 'all of': 23,\n",
       " 'and not': 45,\n",
       " 'sister': 704,\n",
       " 'few': 266,\n",
       " 'world': 971,\n",
       " 'anything': 61,\n",
       " 'usual': 894,\n",
       " 'having': 345,\n",
       " 'perfect': 603,\n",
       " 'song': 716,\n",
       " 'experience': 252,\n",
       " 'for his': 285,\n",
       " 'the world': 804,\n",
       " 'they have': 818,\n",
       " 'are the': 66,\n",
       " 'saw': 656,\n",
       " 'etc': 235,\n",
       " 'happen': 332,\n",
       " 'effects': 219,\n",
       " 'saw this': 657,\n",
       " 'br this': 124,\n",
       " 'to this': 858,\n",
       " 'of these': 566,\n",
       " 'worst': 973,\n",
       " 'shot': 691,\n",
       " 'dark': 180,\n",
       " 'parts': 599,\n",
       " 'directed': 195,\n",
       " 'should': 693,\n",
       " 'budget': 127,\n",
       " 'couldn': 173,\n",
       " 'the worst': 805,\n",
       " 'should have': 695,\n",
       " 'for some': 288,\n",
       " 'much better': 526,\n",
       " 'for this': 290,\n",
       " 'when he': 940,\n",
       " 'of their': 564,\n",
       " 'the only': 787,\n",
       " 'see the': 670,\n",
       " 've': 895,\n",
       " 'special': 721,\n",
       " 'production': 628,\n",
       " 'takes': 744,\n",
       " 'stars': 724,\n",
       " 'quality': 630,\n",
       " 'performances': 605,\n",
       " 'blood': 111,\n",
       " 'david': 181,\n",
       " 'went': 933,\n",
       " 'hour': 368,\n",
       " 'drama': 210,\n",
       " 'shown': 697,\n",
       " 'known': 450,\n",
       " 'could be': 171,\n",
       " 've seen': 896,\n",
       " 'back to': 84,\n",
       " 'but he': 129,\n",
       " 'is great': 406,\n",
       " 'film that': 271,\n",
       " 'absolutely': 7,\n",
       " 'future': 302,\n",
       " 'together': 861,\n",
       " 'do not': 200,\n",
       " 'is good': 405,\n",
       " 'able': 1,\n",
       " 'city': 159,\n",
       " 'near': 535,\n",
       " 'sorry': 718,\n",
       " 'half': 330,\n",
       " 'easy': 218,\n",
       " 'music': 529,\n",
       " 'able to': 2,\n",
       " 'should be': 694,\n",
       " 'you ll': 994,\n",
       " 'you re': 995,\n",
       " 'about the': 4,\n",
       " 'out the': 592,\n",
       " 'the music': 785,\n",
       " 'seeing': 671,\n",
       " 'documentary': 201,\n",
       " 'otherwise': 588,\n",
       " 'moments': 507,\n",
       " 'during': 211,\n",
       " 'the most': 783,\n",
       " 'during the': 212,\n",
       " 'wrong': 983,\n",
       " 'between': 105,\n",
       " 'took': 865,\n",
       " 'doing': 205,\n",
       " 'truly': 872,\n",
       " 'small': 706,\n",
       " 'with this': 962,\n",
       " 'fine': 279,\n",
       " 'cinematography': 158,\n",
       " 'played': 613,\n",
       " 'between the': 106,\n",
       " 'played by': 614,\n",
       " 'of all': 556,\n",
       " 'actress': 15,\n",
       " 'seriously': 684,\n",
       " 'lack': 451,\n",
       " 'reason': 639,\n",
       " 'as an': 70,\n",
       " 'not only': 547,\n",
       " 'side': 699,\n",
       " 'major': 489,\n",
       " 'full': 299,\n",
       " 'start': 725,\n",
       " 'easily': 217,\n",
       " 'themselves': 808,\n",
       " 'piece': 609,\n",
       " 'truth': 873,\n",
       " 'are not': 65,\n",
       " 'the other': 789,\n",
       " 'of it': 561,\n",
       " 'piece of': 610,\n",
       " 'violence': 905,\n",
       " 'sense': 679,\n",
       " 'itself': 431,\n",
       " 'and then': 49,\n",
       " 'and this': 52,\n",
       " 'the characters': 770,\n",
       " 'sense of': 680,\n",
       " 'mother': 515,\n",
       " 'live': 471,\n",
       " 'comedy': 165,\n",
       " 'instead': 395,\n",
       " 'film and': 269,\n",
       " 'of my': 562,\n",
       " 'kid': 440,\n",
       " 'boy': 115,\n",
       " 'monster': 509,\n",
       " '10': 0,\n",
       " 'it the': 426,\n",
       " 'exactly': 247,\n",
       " 'group': 324,\n",
       " 'roles': 648,\n",
       " 'yes': 987,\n",
       " 'modern': 505,\n",
       " 'myself': 533,\n",
       " 'entertaining': 231,\n",
       " 'plenty': 617,\n",
       " 'gore': 321,\n",
       " 'of an': 557,\n",
       " 'special effects': 722,\n",
       " 'bit of': 109,\n",
       " 'through the': 838,\n",
       " 'if they': 381,\n",
       " 'happy': 335,\n",
       " 'musical': 530,\n",
       " 'thinking': 825,\n",
       " 'chance': 150,\n",
       " 'opening': 582,\n",
       " 'predictable': 624,\n",
       " 'it and': 419,\n",
       " 'in his': 388,\n",
       " 'of her': 559,\n",
       " 'crime': 178,\n",
       " 'possible': 622,\n",
       " 'sort': 719,\n",
       " 'keep': 438,\n",
       " 'the viewer': 801,\n",
       " 'sort of': 720,\n",
       " 'there was': 813,\n",
       " 'women': 965,\n",
       " 'interest': 397,\n",
       " 'says': 660,\n",
       " 'water': 925,\n",
       " 'reality': 637,\n",
       " 'town': 868,\n",
       " 'getting': 310,\n",
       " 'and there': 50,\n",
       " 'instead of': 396,\n",
       " 'happened': 333,\n",
       " 'the dvd': 772,\n",
       " 'there were': 814,\n",
       " 'in my': 390,\n",
       " 'film the': 272,\n",
       " 'film was': 274,\n",
       " 'not even': 546,\n",
       " 'coming': 167,\n",
       " 'disappointed': 198,\n",
       " 'none': 544,\n",
       " 'but this': 135,\n",
       " 'enjoyed': 228,\n",
       " 'year': 984,\n",
       " 'year old': 985,\n",
       " 'and so': 46,\n",
       " 'watch it': 921,\n",
       " 'movie but': 519,\n",
       " 'fun': 300,\n",
       " 'appears': 63,\n",
       " 'class': 160,\n",
       " 'throughout': 839,\n",
       " 'loved': 485,\n",
       " 'writing': 981,\n",
       " 'in all': 385,\n",
       " 'sad': 652,\n",
       " 'be the': 89,\n",
       " 'kept': 439,\n",
       " 'often': 571,\n",
       " 'actors': 14,\n",
       " 'close': 163,\n",
       " 'george': 306,\n",
       " 'james': 433,\n",
       " 'poor': 621,\n",
       " 'the actors': 765,\n",
       " 'the real': 791,\n",
       " 'use': 892,\n",
       " 'footage': 283,\n",
       " 'mr': 524,\n",
       " 'rating': 633,\n",
       " 'br if': 119,\n",
       " 'better than': 104,\n",
       " 'was very': 917,\n",
       " 'had to': 329,\n",
       " 'to go': 848,\n",
       " 'to me': 853,\n",
       " 'write': 979,\n",
       " 'western': 935,\n",
       " 'hilarious': 357,\n",
       " 'main': 488,\n",
       " 'fans': 259,\n",
       " 'child': 155,\n",
       " 'hand': 331,\n",
       " 'change': 151,\n",
       " 'stupid': 736,\n",
       " 'well as': 932,\n",
       " 'know what': 449,\n",
       " 'if the': 380,\n",
       " 'as it': 72,\n",
       " 'type': 882,\n",
       " 'lee': 459,\n",
       " 'jack': 432,\n",
       " 'the two': 800,\n",
       " 'looked': 478,\n",
       " 'the main': 781,\n",
       " 'country': 174,\n",
       " 'run': 650,\n",
       " 'important': 383,\n",
       " 'episode': 233,\n",
       " 'team': 748,\n",
       " 'soon': 717,\n",
       " 'the show': 797,\n",
       " 'mind': 503,\n",
       " 'animation': 55,\n",
       " 'human': 373,\n",
       " 'form': 291,\n",
       " 'clearly': 162,\n",
       " 'entire': 232,\n",
       " 'novel': 551,\n",
       " 'the entire': 775,\n",
       " 'after the': 18,\n",
       " 'four': 293,\n",
       " 'living': 473,\n",
       " 'but they': 134,\n",
       " 'will be': 956,\n",
       " 'the original': 788,\n",
       " 'you know': 993,\n",
       " 'in their': 392,\n",
       " 'ideas': 377,\n",
       " 'guys': 327,\n",
       " 'brilliant': 125,\n",
       " 'cannot': 144,\n",
       " 'lead': 456,\n",
       " 'annoying': 56,\n",
       " 'stop': 729,\n",
       " 'ways': 927,\n",
       " 'even the': 238,\n",
       " 'tries to': 870,\n",
       " 'power': 623,\n",
       " 'boys': 116,\n",
       " 'humor': 374}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec_bigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_df = pd.DataFrame(data=countvec_bigram.vocabulary_.values(), index=countvec_bigram.vocabulary_.keys(), columns = ['frequency'])\n",
    "# vocab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aside: Google n-gram viewer\n",
    "\n",
    "[Google n-gram viewer](https://books.google.com/ngrams)\n",
    "\n",
    "<img src=\"img/google_ngram_viewer.png\" width=\"800\" height=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings (15 min)\n",
    "\n",
    "- Word embeddings: \"embed\" a word in a vector space.\n",
    "- You have a bunch of feature columns, each word has a representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Vector space model\n",
    "\n",
    "- Model the meaning of a word by placing it into a vector space.  \n",
    "- Distances among words in the vector space indicate the relationship between them. \n",
    "\n",
    "<img src=\"img/t-SNE_word_embeddings.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "(Attribution: Jurafsky and Martin 3rd edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Distributional hypothesis\n",
    "\n",
    "<blockquote> \n",
    "    <p>You shall know a word by the company it keeps.</p>\n",
    "    <footer>Firth, 1957</footer>        \n",
    "</blockquote>\n",
    "\n",
    "<blockquote> \n",
    "If A and B have almost identical environments we say that they are synonyms.\n",
    "<footer>Harris, 1954</footer>    \n",
    "</blockquote>    \n",
    "\n",
    "Example: \n",
    "\n",
    "- Her **child** loves to play in the playground. \n",
    "- Her **kid** loves to play in the playground. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Co-occurrence matrices\n",
    "- A way to represent vectors into a vector space\n",
    "\n",
    "#### Term-document matrix\n",
    "\n",
    "- This like the transpose of `CountVectorizer`.\n",
    "- Each cell is a count of words in the document in that column. \n",
    "- You can describe a document in terms of the frequencies of different words in it. \n",
    "- You can describe a word in terms of its frequency in different documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apricot</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sugar</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walnut</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             D1  D2  D3  D4  D5\n",
       "apricot       0   0   1   0   0\n",
       "big           1   0   1   0   1\n",
       "data          1   0   0   1   1\n",
       "information   1   0   0   1   1\n",
       "sugar         0   1   0   0   0\n",
       "walnut        0   1   0   0   0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['information big data',\n",
    "        'walnut sugar',\n",
    "        'apricot big',\n",
    "        'information data',\n",
    "        'big data information']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "header = vectorizer.get_feature_names()\n",
    "labels = ['D1', 'D2', 'D3', 'D4', 'D5']\n",
    "df = pd.DataFrame(X.toarray(), columns = header, index = labels)  \n",
    "df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-term matrix\n",
    "\n",
    "- The idea is to go through a corpus of text, keeping a count of all of the words that appear in its context within a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apricot</th>\n",
       "      <th>big</th>\n",
       "      <th>data</th>\n",
       "      <th>information</th>\n",
       "      <th>sugar</th>\n",
       "      <th>walnut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apricot</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sugar</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walnut</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             apricot  big  data  information  sugar  walnut\n",
       "apricot            0    1     0            0      0       0\n",
       "big                1    0     2            2      0       0\n",
       "data               0    2     0            3      0       0\n",
       "information        0    2     3            0      0       0\n",
       "sugar              0    0     0            0      0       1\n",
       "walnut             0    0     0            0      1       0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['information big data',\n",
    "        'walnut sugar',\n",
    "        'apricot big',\n",
    "        'information data',\n",
    "        'big data information']\n",
    "vec = CountVectorizer(ngram_range=(1,1)) \n",
    "X = vec.fit_transform(corpus)\n",
    "X_ww = X.T@X\n",
    "X_ww.setdiag(0) \n",
    "header = vec.get_feature_names()\n",
    "ind_col = header\n",
    "df = pd.DataFrame(X_ww.toarray(), columns = header, index = ind_col)  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>big</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sugar</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walnut</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             big  data\n",
       "data           2     0\n",
       "information    2     3\n",
       "sugar          0     0\n",
       "walnut         0     0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset = df[['big','data']]\n",
    "df_subset.iloc[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sparse vs. dense word vectors\n",
    "\n",
    "- Term-term and term-document matrices are sparse. \n",
    "- OK because there are efficient ways to deal with sparse matrices.\n",
    "\n",
    "\n",
    "#### Alternative \n",
    "- Learn short (~100 to 1000 dimensions) and dense vectors. \n",
    "- These short dense representations of words are referred to as **word embeddings**.\n",
    "- Short vectors may be easier to train with ML models (less weights to train).\n",
    "- They may generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we get dense vectors?\n",
    " \n",
    "- Count-based methods\n",
    "    - Singular Value Decomposition (SVD) - beyond the scope of the course\n",
    "- Prediction-based methods\n",
    "    - [Word2Vec](https://github.com/tmikolov/word2vec)\n",
    "    - [fastText](https://fasttext.cc/)\n",
    "    - [GloVe](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "<img src=\"img/word2vec.png\" width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Success of Word2Vec\n",
    "\n",
    "- Able to capture complex relationships between words.\n",
    "- Example: What is the word that is similar to **WOMAN** in the same sense as **KING** is similar to **MAN**?\n",
    "- Perform a simple algebraic operations with the vector representation of words.\n",
    "    $\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance.\n",
    "\n",
    "<img src=\"img/word_analogies1.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "(Credit: Mikolov et al. 2013)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pre-trained embeddings\n",
    "\n",
    "- These embeddings come from training complicated ML models on big data sets.\n",
    "- However, we can skip this step and use **pre-trained** embeddings.\n",
    "- Someone already did `fit` for us and published the results, we can download and just use `transform`.\n",
    "\n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "    * trained on several corpora using the word2vec algorithm \n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using [the fastText algorithm](http://aclweb.org/anthology/Q17-1010)\n",
    "    * published by Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Google's pre-trained Word2Vec model:\n",
    "\n",
    "- Requires download a 1.5 GB file [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary: ', len(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between pineapple and mango is 0.668\n",
      "The similarity between pineapple and juice is 0.418\n",
      "The similarity between sun and robot is 0.029\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [('pineapple','mango'), ('pineapple','juice'), ('sun','robot')]\n",
    "for pair in word_pairs: \n",
    "    print('The similarity between %s and %s is %0.3f' %(pair[0], pair[1], model.similarity(pair[0], pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding similar words\n",
    "Given word $w$, search in the vector space for the word closest to $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# model.most_similar('mango')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UVic', 0.7886475324630737),\n",
       " ('SFU', 0.7588527798652649),\n",
       " ('Simon_Fraser', 0.7356574535369873),\n",
       " ('UFV', 0.6880435943603516),\n",
       " ('VIU', 0.6778583526611328),\n",
       " ('Kwantlen', 0.677142858505249),\n",
       " ('UBCO', 0.6734487414360046),\n",
       " ('UPEI', 0.6731126308441162),\n",
       " ('UBC_Okanagan', 0.6709134578704834),\n",
       " ('Lakehead_University', 0.6622507572174072)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('UBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('info', 0.7363681793212891),\n",
       " ('infomation', 0.6800296306610107),\n",
       " ('infor_mation', 0.6733849048614502),\n",
       " ('informaiton', 0.6639008522033691),\n",
       " ('informa_tion', 0.660125732421875),\n",
       " ('informationon', 0.633933424949646),\n",
       " ('informationabout', 0.6320979595184326),\n",
       " ('Information', 0.6186580061912537),\n",
       " ('informaion', 0.6093292236328125),\n",
       " ('details', 0.6063089370727539)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Finding the odd one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UBC\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"sun moon earth UBC mars\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Distance between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between related sentences 2.2813\n",
      "Distance between unrelated sentences 3.4133\n"
     ]
    }
   ],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "sentence_unrelated = 'Data science is a multidisciplinary blend of data inference, algorithmm development, and technology.'\n",
    "\n",
    "similarity = model.wmdistance(sentence_obama, sentence_president)\n",
    "print(\"Distance between related sentences {:.4f}\".format(similarity))\n",
    "\n",
    "similarity = model.wmdistance(sentence_obama, sentence_unrelated)\n",
    "print(\"Distance between unrelated sentences {:.4f}\".format(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3):\n",
    "    print('%s : %s :: %s : ?' %(word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=['Analogy word', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : ?\n"
     ]
    }
   ],
   "source": [
    "analogy('man','king','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montreal : Canadiens :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canucks</td>\n",
       "      <td>0.821327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vancouver_Canucks</td>\n",
       "      <td>0.750401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary_Flames</td>\n",
       "      <td>0.705470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leafs</td>\n",
       "      <td>0.695783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maple_Leafs</td>\n",
       "      <td>0.691617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thrashers</td>\n",
       "      <td>0.687504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avs</td>\n",
       "      <td>0.681716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sabres</td>\n",
       "      <td>0.665307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Blackhawks</td>\n",
       "      <td>0.664625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Habs</td>\n",
       "      <td>0.661023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Analogy word     Score\n",
       "0            Canucks  0.821327\n",
       "1  Vancouver_Canucks  0.750401\n",
       "2     Calgary_Flames  0.705470\n",
       "3              Leafs  0.695783\n",
       "4        Maple_Leafs  0.691617\n",
       "5          Thrashers  0.687504\n",
       "6                Avs  0.681716\n",
       "7             Sabres  0.665307\n",
       "8         Blackhawks  0.664625\n",
       "9               Habs  0.661023"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Montreal', 'Canadiens', 'Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft : Windows :: Apple : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Macs</td>\n",
       "      <td>0.673568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iMac</td>\n",
       "      <td>0.646340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac_OS</td>\n",
       "      <td>0.640714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>0.640588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPad</td>\n",
       "      <td>0.633464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OS_X</td>\n",
       "      <td>0.632136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iBook</td>\n",
       "      <td>0.626197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iMacs</td>\n",
       "      <td>0.619245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iOS</td>\n",
       "      <td>0.617178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mac_mini</td>\n",
       "      <td>0.611140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0         Macs  0.673568\n",
       "1         iMac  0.646340\n",
       "2       Mac_OS  0.640714\n",
       "3       iPhone  0.640588\n",
       "4         iPad  0.633464\n",
       "5         OS_X  0.632136\n",
       "6        iBook  0.626197\n",
       "7        iMacs  0.619245\n",
       "8          iOS  0.617178\n",
       "9     Mac_mini  0.611140"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Microsoft', 'Windows', 'Apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauss : mathematician :: Socrates : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>philosopher</td>\n",
       "      <td>0.540793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Socrates_Plato</td>\n",
       "      <td>0.478897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>philosopher_Aristotle</td>\n",
       "      <td>0.467387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>philosopher_Socrates</td>\n",
       "      <td>0.459890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>0.455209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Philosopher</td>\n",
       "      <td>0.452536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logician</td>\n",
       "      <td>0.448070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nobel_laureate_José_Saramago</td>\n",
       "      <td>0.444382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poet</td>\n",
       "      <td>0.443180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Plato_Socrates</td>\n",
       "      <td>0.441025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Analogy word     Score\n",
       "0                   philosopher  0.540793\n",
       "1                Socrates_Plato  0.478897\n",
       "2         philosopher_Aristotle  0.467387\n",
       "3          philosopher_Socrates  0.459890\n",
       "4                     Aristotle  0.455209\n",
       "5                   Philosopher  0.452536\n",
       "6                      logician  0.448070\n",
       "7  Nobel_laureate_José_Saramago  0.444382\n",
       "8                          poet  0.443180\n",
       "9                Plato_Socrates  0.441025"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Gauss', 'mathematician', 'Socrates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Implicit biases and stereotypes in word embeddings\n",
    "\n",
    "- Reflect gender stereotypes present in broader society\n",
    "- They may also amplify these stereotypes because of their widespread usage \n",
    "- See [this paper](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : computer_programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similar word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.562712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.510505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphic_designer</td>\n",
       "      <td>0.505180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.497949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.493489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.492551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>0.490797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saleswoman</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electrical_engineer</td>\n",
       "      <td>0.479773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanical_engineer</td>\n",
       "      <td>0.475540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Similar word     Score\n",
       "0            homemaker  0.562712\n",
       "1            housewife  0.510505\n",
       "2     graphic_designer  0.505180\n",
       "3        schoolteacher  0.497949\n",
       "4        businesswoman  0.493489\n",
       "5            paralegal  0.492551\n",
       "6     registered_nurse  0.490797\n",
       "7           saleswoman  0.488163\n",
       "8  electrical_engineer  0.479773\n",
       "9  mechanical_engineer  0.475540"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'computer_programmer', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next class: we'll use these word embedding features for supervised learning, compare to `CountVectorizer` and `TfidfVectorizer`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
